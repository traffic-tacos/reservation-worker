---
description:
globs:
alwaysApply: true
---
당신은 Go로 reservation-worker(worker)를 구현하는 시니어 서버 엔지니어입니다.  
이 서비스는 예약 만료/결제 결과 등 비동기 이벤트를 처리하는 워크플로우 실행기입니다.  
인프라/배포(K8s/Helm/IaC)는 이미 준비되어 있으며, 오토스케일링은 EKS에서 **KEDA**가 SQS backlog를 기반으로 수행합니다.  
따라서 애플리케이션은 이벤트 소비/처리 로직, 멱등성, 관측/테스트에만 집중하세요.

## 0) 컨텍스트
- 주요 역할:
  - EventBridge/SQS에 쌓인 이벤트를 poll & consume
  - "예약 만료 → 좌석/재고 복구"
  - "결제 승인/실패 → 예약 상태 확정 or 해제"
- 이벤트 원본: reservation-api, payment-sim-api
- 통신 대상: inventory-svc(gRPC), reservation-api(REST/gRPC)

## 1) 기술 스택
- Go 1.22+, linux/arm64
- 메시지 소비: AWS SDK v2 SQS client
- gRPC client: inventory-svc
- REST/gRPC client: reservation-api
- 직렬화: JSON
- 로깅: zap(JSON)
- 관측: OpenTelemetry(OTLP), Prometheus(client_golang)
- 동시성 처리: goroutines, worker pool
- KEDA 전제: Pod 수는 큐 backlog에 따라 자동 증가/감소.  
  (앱 레벨에서는 단일 인스턴스에서의 동시성 안전성만 확보)

## 2) 이벤트 페이로드 스키마
모든 이벤트는 JSON, 최소한 아래 공통 필드를 가짐:
{
  "type": "reservation.expired | payment.approved | payment.failed",
  "reservation_id": "rsv_123",
  "event_id": "evt_456",
  "ts": "ISO8601",
  "payload": { ... }   // 타입별 추가 필드
}

- reservation.expired
  payload: { "qty": 2, "seat_ids": ["A1","A2"] }
- payment.approved
  payload: { "payment_intent_id":"pay_abc", "amount":120000 }
- payment.failed
  payload: { "payment_intent_id":"pay_def", "amount":120000 }

## 3) 워크플로우 동작
- reservation.expired:
  1) inventory-svc gRPC 호출 → ReleaseHold(event_id, reservation_id, qty/seat_ids)
  2) reservation-api 상태 업데이트(예약=EXPIRED)
- payment.approved:
  1) reservation-api 상태=CONFIRMED 갱신
  2) (선택) inventory-svc CommitReservation으로 SOLD 확정
- payment.failed:
  1) reservation-api 상태=CANCELLED 갱신
  2) inventory-svc ReleaseHold으로 재고 복구

## 4) 통신 계약
### inventory-svc (gRPC)
- ReleaseHold(ReleaseReq) → ReleaseRes
- CommitReservation(CommitReq) → CommitRes

### reservation-api (REST)
- PATCH /internal/reservations/{reservation_id}
  Body: { "status":"CONFIRMED|CANCELLED|EXPIRED" }

## 5) 동시성/성능 목표
- 단일 인스턴스 기준:
  - worker pool: 20 goroutines (env 조정 가능)
  - 큐 polling은 long polling(20s)
  - P95 이벤트 처리 지연 < 200ms
- Pod 수 자체는 KEDA가 SQS backlog에 맞춰 오토스케일링(min=0, max=50)  
  → 앱은 스케일아웃에 안전하도록 **stateless 설계**.

## 6) 장애/에러 처리
- 외부 호출 실패 시 재시도(backoff 1s, 2s, 4s, 8s, max 5회)
- 실패 횟수 초과 → DLQ (SQS 설정에 위임), 앱 레벨에서는 에러 로그 + 메트릭 증가
- 멱등성: reservation-api, inventory-svc 호출은 reservation_id 기반 멱등 보장 → 재시도 안전

## 7) 관측/로그/메트릭
- OpenTelemetry: traceparent 수용/전파 (이벤트→후속 호출 span)
- Prometheus 지표:
  - worker_events_total{type, outcome}
  - worker_latency_seconds_bucket{type}
  - sqs_poll_errors_total
- zap JSON 로그:
  ts, level, event_type, reservation_id, attempt, outcome, latency_ms, trace_id, pod_name

## 8) 설정(환경변수 키만; 값 주입은 외부)
- SQS_QUEUE_URL=https://sqs.ap-northeast-2.amazonaws.com/123/queue
- SQS_WAIT_TIME=20
- WORKER_CONCURRENCY=20
- MAX_RETRIES=5
- BACKOFF_BASE_MS=1000
- INVENTORY_GRPC_ADDR=inventory-svc:8080
- RESERVATION_API_BASE=http://reservation-api:8080
- OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
- LOG_LEVEL=info

## 9) 테스트/품질
- 단위: 이벤트 파싱, 라우팅, 재시도 로직
- 통합: LocalStack SQS + 가짜 inventory-svc/reservation-api로 end-to-end
- 시뮬: expired + approved + failed 이벤트 시나리오
- 부하: 초당 수천 이벤트를 KEDA가 Pod scale-out으로 처리하는지 시뮬 (앱은 단일 Pod 안정성만 검증)

## 10) 리포지토리 구조
- /cmd/reservation-worker/main.go
- /internal/worker/   // sqs poller, dispatcher
- /internal/handler/  // 이벤트 타입별 처리
- /internal/client/   // inventory-svc gRPC, reservation-api REST
- /internal/observability/ // otel, prometheus, logger
- /internal/config/   // env 파서
- /test/              // unit + integration
- Makefile, Dockerfile, README.md

## 11) 에러 outcome 값
- success, retried, failed, dropped
- InvalidPayload, DownstreamError

### 최종 지시
위 명세에 따라 reservation-worker의 **애플리케이션 코드/계약/관측/테스트**를 생성하세요.  
인프라/배포(K8s/Helm/IaC, KEDA 리소스 정의)는 포함하지 말고,  
애플리케이션은 **stateless, 큐 backlog 기반 오토스케일링(KEDA 전제)**에 안전하게 동작하도록 하세요.