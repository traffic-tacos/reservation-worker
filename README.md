# ğŸ« Reservation Worker

> **Cloud-Native Event-Driven Background Processor for High-Traffic Reservation Systems**

ëŒ€ê·œëª¨ í‹°ì¼“ ì˜ˆì•½ ì‹œìŠ¤í…œì„ ìœ„í•œ ì´ë²¤íŠ¸ ê¸°ë°˜ ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.  
SQS + EventBridgeë¥¼ í™œìš©í•œ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ì²˜ë¦¬ì™€ KEDA ê¸°ë°˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ **30k RPS íŠ¸ë˜í”½**ì„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

[![Go Version](https://img.shields.io/badge/Go-1.24+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![AWS SDK](https://img.shields.io/badge/AWS_SDK-v2-FF9900?style=flat&logo=amazon-aws)](https://aws.github.io/aws-sdk-go-v2/)
[![gRPC](https://img.shields.io/badge/gRPC-Proto--Contracts-4285F4?style=flat&logo=grpc)](https://grpc.io/)
[![KEDA](https://img.shields.io/badge/KEDA-Autoscaling-326CE5?style=flat&logo=kubernetes)](https://keda.sh/)

---

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#-í”„ë¡œì íŠ¸-ê°œìš”)
- [í•µì‹¬ íŠ¹ì§•](#-í•µì‹¬-íŠ¹ì§•)
- [ì•„í‚¤í…ì²˜ ì„¤ê³„](#-ì•„í‚¤ï¿½Ã©kì²˜-ì„¤ê³„)
- [ê¸°ìˆ  ìŠ¤íƒ & ì„¤ê³„ ê²°ì •](#-ê¸°ìˆ -ìŠ¤íƒ--ì„¤ê³„-ê²°ì •)
- [Quick Start](#-quick-start)
- [ì´ë²¤íŠ¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°](#-ì´ë²¤íŠ¸-ì²˜ë¦¬-ì›Œí¬í”Œë¡œìš°)
- [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
- [ê´€ì¸¡ì„± & ëª¨ë‹ˆí„°ë§](#-ê´€ì¸¡ì„±--ëª¨ë‹ˆí„°ë§)
- [ë°°í¬ ì „ëµ](#-ë°°í¬-ì „ëµ)
- [ê°œë°œ ê°€ì´ë“œ](#-ê°œë°œ-ê°€ì´ë“œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

**Reservation Worker**ëŠ” Traffic Tacos ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ê³„ì¸µì…ë‹ˆë‹¤.

### ì£¼ìš” ì±…ì„

| ì´ë²¤íŠ¸ íƒ€ì… | ì²˜ë¦¬ ë‚´ìš© | ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ |
|---------|--------|---------|
| **reservation.expired** | 60ì´ˆ hold ì‹œê°„ ë§Œë£Œ ì‹œ ì¬ê³  ìë™ ë³µêµ¬ | ì˜¤ë²„ì…€ ë°©ì§€, ì¬ê³  íš¨ìœ¨ì„± í–¥ìƒ |
| **payment.approved** | ê²°ì œ ì„±ê³µ ì‹œ ì˜ˆì•½ í™•ì • & ì¬ê³  SOLD ì²˜ë¦¬ | ì£¼ë¬¸ í™•ì •, ë§¤ì¶œ ì‹¤í˜„ |
| **payment.failed** | ê²°ì œ ì‹¤íŒ¨ ì‹œ ì˜ˆì•½ ì·¨ì†Œ & ì¬ê³  ë³µêµ¬ | ì¬ê³  ê°€ìš©ì„± íšŒë³µ, ë³´ìƒ íŠ¸ëœì­ì…˜ |

### ì™œ Event-Driven ì•„í‚¤í…ì²˜ì¸ê°€?

**ë™ê¸° ì²˜ë¦¬ì˜ í•œê³„:**
- 30k RPS íŠ¸ë˜í”½ ì‹œ Downstream ì„œë¹„ìŠ¤(inventory, payment) ë³‘ëª© ë°œìƒ
- íƒ€ì„ì•„ì›ƒ/ì¬ì‹œë„ë¡œ ì¸í•œ ì‚¬ìš©ì ê²½í—˜ ì €í•˜
- ê²°í•©ë„ ì¦ê°€ë¡œ ì¥ì•  ì „íŒŒ ìœ„í—˜

**ë¹„ë™ê¸° ì´ë²¤íŠ¸ ê¸°ë°˜ í•´ë²•:**
```
User Request â†’ Reservation API (ì¦‰ì‹œ ì‘ë‹µ, 202 Accepted)
                    â†“
              EventBridge/SQS (ì´ë²¤íŠ¸ ë²„í¼ë§)
                    â†“
          Reservation Worker (Pool ì²˜ë¦¬, KEDA ìŠ¤ì¼€ì¼)
                    â†“
        Downstream Services (ë¶€í•˜ ë¶„ì‚°, ì¬ì‹œë„ ì•ˆì „)
```

**í•µì‹¬ ì´ì :**
- ğŸš€ **ì²˜ë¦¬ëŸ‰ í–¥ìƒ**: ì›Œì»¤ í’€ ê¸°ë°˜ ë™ì‹œ ì²˜ë¦¬ (ê¸°ë³¸ 20 goroutines)
- ğŸ”„ **ì¥ì•  ê²©ë¦¬**: ì´ë²¤íŠ¸ íë¥¼ í†µí•œ ì„œë¹„ìŠ¤ ê°„ ë””ì»¤í”Œë§
- ğŸ“ˆ **íƒ„ë ¥ì  í™•ì¥**: KEDAê°€ SQS backlog ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ (0â†’50 pods)
- ğŸ” **ì¬ì‹œë„ ì•ˆì „**: Exponential backoff + ë©±ë“±ì„± ë³´ì¥
- ğŸ“Š **ê°€ì‹œì„±**: ë¶„ì‚° ì¶”ì , êµ¬ì¡°í™” ë¡œê¹…, ë©”íŠ¸ë¦­ ìˆ˜ì§‘

---

## âœ¨ í•µì‹¬ íŠ¹ì§•

### 1ï¸âƒ£ **Event-Driven Architecture**
- âœ… **3ê°€ì§€ ì´ë²¤íŠ¸ íƒ€ì…** ì²˜ë¦¬ (expired, approved, failed)
- âœ… **EventBridge â†’ SQS** í†µí•©ìœ¼ë¡œ ë‚´êµ¬ì„± ìˆëŠ” ì´ë²¤íŠ¸ ì „ë‹¬
- âœ… **DLQ (Dead Letter Queue)** ì§€ì›ìœ¼ë¡œ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë¶„ë¦¬
- âœ… **ë°°ì¹˜ ì²˜ë¦¬**: í•œ ë²ˆì— ìµœëŒ€ 10ê°œ ë©”ì‹œì§€ ë™ì‹œ ìˆ˜ì‹ 

### 2ï¸âƒ£ **Cloud-Native Resilience**
- âœ… **Exponential Backoff Retry**: `1s â†’ 2s â†’ 4s â†’ 8s â†’ 16s` (ìµœëŒ€ 5íšŒ)
- âœ… **ë©±ë“±ì„± ë³´ì¥**: reservation_id ê¸°ë°˜ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
- âœ… **Graceful Shutdown**: 30ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì§„í–‰ ì¤‘ ì‘ì—… ì™„ë£Œ
- âœ… **Circuit Breaker íŒ¨í„´**: gRPC/REST í´ë¼ì´ì–¸íŠ¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •

### 3ï¸âƒ£ **Proto-Contracts í†µí•©**
```go
// ì¤‘ì•™í™”ëœ proto-contracts ëª¨ë“ˆ ì‚¬ìš©
import "github.com/traffic-tacos/proto-contracts/gen/go/reservation/v1"

// gRPC í´ë¼ì´ì–¸íŠ¸ ì¼ê´€ì„±
inventoryClient.ReleaseHold(ctx, &reservationv1.ReleaseHoldRequest{
    EventId:       "evt_123",
    ReservationId: "rsv_456",
    Quantity:      2,
    SeatIds:       []string{"A1", "A2"},
})
```
**ì´ì :**
- ğŸ”— ì„œë¹„ìŠ¤ ê°„ API ê³„ì•½ ë²„ì „ ê´€ë¦¬
- ğŸ›¡ï¸ Type-safe gRPC í†µì‹ 
- ğŸ”„ Proto ì •ì˜ ë³€ê²½ ì‹œ ìë™ ê°ì§€ (build failure)

### 4ï¸âƒ£ **KEDA Auto-Scaling**
```yaml
# Kubernetes ScaledObject ì˜ˆì‹œ
triggers:
- type: aws-sqs-queue
  metadata:
    queueURL: ${SQS_QUEUE_URL}
    queueLength: "10"  # ë©”ì‹œì§€ 10ê°œë‹¹ 1 pod
    awsRegion: "ap-northeast-2"
```
**ìŠ¤ì¼€ì¼ë§ ë™ì‘:**
- ğŸ“‰ **Scale-to-Zero**: ì´ë²¤íŠ¸ ì—†ìœ¼ë©´ pod 0ê°œ (ë¹„ìš© ì ˆê°)
- ğŸ“ˆ **ê¸‰ê²©í•œ íŠ¸ë˜í”½ ì¦ê°€**: í backlog ê¸°ë°˜ ì¦‰ì‹œ í™•ì¥ (max 50 pods)
- âš–ï¸ **ì•ˆì •í™”**: í ì†Œì§„ ì‹œ ì ì§„ì  ì¶•ì†Œ

### 5ï¸âƒ£ **Multi-Strategy AWS Authentication**
```go
// 1ï¸âƒ£ IRSA (EKS Pod Identity) - ìš´ì˜ í™˜ê²½ ê¶Œì¥
// IAM Role for Service Account ìë™ ì¸ì¦

// 2ï¸âƒ£ Named Profile - ë¡œì»¬ ê°œë°œ
AWS_PROFILE=tacos

// 3ï¸âƒ£ Static Credentials - CI/CD íŒŒì´í”„ë¼ì¸
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
```

### 6ï¸âƒ£ **Developer Experience ì¤‘ì‹œ**
- ğŸ› ï¸ **grpcui í†µí•©**: í¬íŠ¸ 8041ì—ì„œ gRPC ë””ë²„ê¹… ì¸í„°í˜ì´ìŠ¤
- ğŸ“‹ **Comprehensive Makefile**: 50+ ë¹Œë“œ/í…ŒìŠ¤íŠ¸/ë°°í¬ ëª…ë ¹
- ğŸ³ **Multi-stage Dockerfile**: ìµœì¢… ì´ë¯¸ì§€ í¬ê¸° ~15MB
- ğŸ“ **Structured Logging**: JSON í˜•íƒœ, trace_id ìë™ ì „íŒŒ
- ğŸ“Š **Prometheus Metrics**: RED ë©”íŠ¸ë¦­ (Rate, Errors, Duration)

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     EventBridge     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reservation API â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   SQS Queue  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     (Publish)       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â”‚ Long Polling
                                               â”‚ (20s wait)
                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Reservation Worker                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Poller    â”‚â†’ â”‚ Dispatcher â”‚â†’ â”‚ Worker Poolâ”‚           â”‚
â”‚  â”‚ (SQS SDK)  â”‚  â”‚ (Routing)  â”‚  â”‚ (20 gorout)â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                          â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Event Handlers              â”‚              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚
â”‚  â”‚  â”‚ExpiredHandlerâ”‚  â”‚ApprovedHandlerâ”‚ â”‚ FailedHandlerâ”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                   â”‚                â”‚
             â”‚ gRPC              â”‚ REST           â”‚ gRPC
             â–¼                   â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Inventory API   â”‚  â”‚Reservation APIâ”‚  â”‚Inventory API â”‚
    â”‚ (ReleaseHold)    â”‚  â”‚(UpdateStatus) â”‚  â”‚(CommitRes)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Flow

**1. SQS Poller** (Long Polling)
```go
// 20ì´ˆ ëŒ€ê¸°ë¡œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìµœì†Œí™”
result, _ := sqsClient.ReceiveMessage(ctx, &sqs.ReceiveMessageInput{
    QueueUrl:            queueURL,
    MaxNumberOfMessages: 10,  // ë°°ì¹˜ ì²˜ë¦¬
    WaitTimeSeconds:     20,  // Long polling
})
```

**2. Dispatcher** (Event Routing)
```go
// ì´ë²¤íŠ¸ íƒ€ì…ë³„ í•¸ë“¤ëŸ¬ ë¼ìš°íŒ…
switch event.Type {
case "reservation.expired":
    handler = expiredHandler
case "payment.approved":
    handler = approvedHandler
case "payment.failed":
    handler = failedHandler
}

// Exponential backoff retry ì ìš©
retryer.Do(ctx, "handle_event", func(ctx context.Context) error {
    return handler.Handle(ctx, event)
})
```

**3. Worker Pool** (Concurrent Processing)
```go
// 20ê°œ goroutineì´ ë™ì‹œì— ì´ë²¤íŠ¸ ì²˜ë¦¬
for i := 0; i < workerConcurrency; i++ {
    go func() {
        for event := range eventsChan {
            dispatcher.Dispatch(ctx, event)
        }
    }()
}
```

### Retry & Idempotency Strategy

**Exponential Backoff:**
```
Attempt 1: 1s delay  â”€â”
Attempt 2: 2s delay   â”œâ”€ Max 5 attempts
Attempt 3: 4s delay   â”‚
Attempt 4: 8s delay   â”‚
Attempt 5: 16s delay â”€â”˜
   â†“
  DLQ (Dead Letter Queue)
```

**ë©±ë“±ì„± ë³´ì¥:**
- **reservation_id ê¸°ë°˜**: ë™ì¼ ì´ë²¤íŠ¸ ì¬ì²˜ë¦¬ ì‹œ Downstream ì„œë¹„ìŠ¤ê°€ ë©±ë“± ë³´ì¥
- **inventory-svc**: DynamoDB conditional writeë¡œ ì¤‘ë³µ ReleaseHold ë°©ì§€
- **reservation-api**: ìƒíƒœ ì „ì´ ê²€ì¦ (HOLD â†’ EXPIREDë§Œ í—ˆìš©)

---

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ & ì„¤ê³„ ê²°ì •

### Core Technologies

| ê¸°ìˆ  | ë²„ì „ | ì„ íƒ ì´ìœ  |
|-----|-----|--------|
| **Go** | 1.24 | âœ… Goroutine ê¸°ë°˜ ê²½ëŸ‰ ë™ì‹œì„±<br>âœ… gRPC ë„¤ì´í‹°ë¸Œ ì§€ì›<br>âœ… ë¹ ë¥¸ ì»´íŒŒì¼ & ì‘ì€ ë°”ì´ë„ˆë¦¬ í¬ê¸° |
| **AWS SDK Go v2** | Latest | âœ… Context ê¸°ë°˜ ì·¨ì†Œ ê°€ëŠ¥ ìš”ì²­<br>âœ… IRSA ë„¤ì´í‹°ë¸Œ ì§€ì›<br>âœ… ì„±ëŠ¥ ê°œì„  (v1 ëŒ€ë¹„ 30% ë¹ ë¦„) |
| **gRPC** | v1.60+ | âœ… HTTP/2 ë©€í‹°í”Œë ‰ì‹±<br>âœ… Protobuf ì§ë ¬í™” (JSON ëŒ€ë¹„ 3-10ë°° ë¹ ë¦„)<br>âœ… Streaming ì§€ì› (ì¶”í›„ í™•ì¥) |
| **Proto-Contracts** | Central Module | âœ… API ê³„ì•½ ì¤‘ì•™ ê´€ë¦¬<br>âœ… ì„œë¹„ìŠ¤ ê°„ íƒ€ì… ì¼ê´€ì„±<br>âœ… ë²„ì „ ê´€ë¦¬ ìš©ì´ |
| **OpenTelemetry** | v1.21+ | âœ… ë²¤ë” ì¤‘ë¦½ì  ê´€ì¸¡ì„± í‘œì¤€<br>âœ… Trace/Metric/Log í†µí•©<br>âœ… ë¶„ì‚° ì¶”ì  ìë™ ì „íŒŒ |
| **Prometheus** | Client v1.18 | âœ… K8s í‘œì¤€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘<br>âœ… PromQL ê°•ë ¥í•œ ì¿¼ë¦¬<br>âœ… Grafana ë„¤ì´í‹°ë¸Œ í†µí•© |

### Key Design Decisions

#### 1ï¸âƒ£ **ì™œ Worker Pool íŒ¨í„´ì¸ê°€?**

**ë¹„êµ: Thread-per-Message vs Worker Pool**

```go
âŒ Thread-per-Message (ì•ˆí‹°íŒ¨í„´)
for msg := range sqsMessages {
    go processMessage(msg)  // ë¬´ì œí•œ goroutine ìƒì„±
}
// ë¬¸ì œ: ë©”ëª¨ë¦¬ ê³ ê°ˆ, ìŠ¤ì¼€ì¤„ë§ ì˜¤ë²„í—¤ë“œ

âœ… Worker Pool (í˜„ì¬ êµ¬ì¡°)
eventsChan := make(chan *Event, 100)  // ë²„í¼ ì±„ë„
for i := 0; i < 20; i++ {
    go worker(eventsChan)  // ê³ ì • 20ê°œ goroutine
}
// ì´ì : ë¦¬ì†ŒìŠ¤ ì œì–´, ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥
```

**ì„±ëŠ¥ ë¶„ì„:**
- **ë©”ëª¨ë¦¬**: ê³ ì • ~40MB (vs ë¬´ì œí•œ ì¦ê°€)
- **ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ ~200 ì´ë²¤íŠ¸ (ë‹¨ì¼ pod ê¸°ì¤€)
- **ë ˆì´í„´ì‹œ**: P95 < 120ms (Downstream í¬í•¨)

#### 2ï¸âƒ£ **ì™œ Long Pollingì¸ê°€?**

**ë¹„êµ: Short Polling vs Long Polling**

| ë°©ì‹ | API ìš”ì²­ íšŸìˆ˜ (1ë¶„) | ë¹„ìš© | ë ˆì´í„´ì‹œ |
|-----|-----------------|-----|-------|
| Short Polling (1ì´ˆ) | 60íšŒ | ë†’ìŒ | ~500ms |
| Long Polling (20ì´ˆ) | 3íšŒ | ë‚®ìŒ | ~100ms |

```go
// Long Polling ì„¤ì •
WaitTimeSeconds: 20  // SQS APIì— 20ì´ˆ ëŒ€ê¸° ì§€ì‹œ
```

**ì´ì :**
- ğŸ’° **ë¹„ìš© ì ˆê°**: API ìš”ì²­ 95% ê°ì†Œ
- âš¡ **ë¹ ë¥¸ ì‘ë‹µ**: ì´ë²¤íŠ¸ ë„ì°© ì¦‰ì‹œ ìˆ˜ì‹ 
- ğŸŒ **ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨**: ë¶ˆí•„ìš”í•œ HTTP ì˜¤ë²„í—¤ë“œ ì œê±°

#### 3ï¸âƒ£ **ì™œ Graceful Shutdownì¸ê°€?**

```go
// SIGTERM ìˆ˜ì‹  ì‹œ 30ì´ˆ ìœ ì˜ˆ ê¸°ê°„
signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
<-sigChan

// 1. ìƒˆ ë©”ì‹œì§€ ìˆ˜ì‹  ì¤‘ë‹¨
poller.Stop()

// 2. ì§„í–‰ ì¤‘ ì´ë²¤íŠ¸ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
wg.Wait()

// 3. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
inventoryClient.Close()
tracerProvider.Shutdown()
```

**K8s Pod ì¢…ë£Œ ì‹œë‚˜ë¦¬ì˜¤:**
```
1. K8s sends SIGTERM
2. Worker stops accepting new events
3. Wait for in-flight events (max 30s)
4. Pod terminates gracefully
   â†“
âœ… Zero event loss
âœ… No half-processed state
```

#### 4ï¸âƒ£ **Multi-Stage Docker Build**

```dockerfile
# Stage 1: Builder (golang:1.24-alpine, ~300MB)
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 go build -ldflags='-w -s' ...

# Stage 2: Runtime (alpine:3.19, ~15MB)
FROM alpine:3.19
COPY --from=builder /app/reservation-worker .
USER appuser
ENTRYPOINT ["./reservation-worker"]
```

**ìµœì í™” ê²°ê³¼:**
- ğŸ“¦ **ì´ë¯¸ì§€ í¬ê¸°**: 300MB â†’ **15MB** (95% ê°ì†Œ)
- ğŸš€ **ë°°í¬ ì†ë„**: Pull ì‹œê°„ 80% ë‹¨ì¶•
- ğŸ”’ **ë³´ì•ˆ**: ìµœì†Œ ê³µê²© í‘œë©´, non-root ì‹¤í–‰

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Go 1.24+ ì„¤ì¹˜
brew install go

# grpcui ì„¤ì¹˜ (gRPC ë””ë²„ê¹…ìš©)
go install github.com/fullstorydev/grpcui/cmd/grpcui@latest

# AWS CLI ì„¤ì •
aws configure --profile tacos
# Region: ap-northeast-2
```

### Local Development

**1ï¸âƒ£ Clone & Setup**
```bash
git clone https://github.com/traffic-tacos/reservation-worker.git
cd reservation-worker

# ì˜ì¡´ì„± ë‹¤ìš´ë¡œë“œ
make init

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
cp .env.example .env.local
# .env.local íŒŒì¼ í¸ì§‘ (AWS credentials, SQS URL ë“±)
```

**2ï¸âƒ£ Run Locally**
```bash
# í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ì‚¬ìš©
make run-with-env

# ë˜ëŠ” ì¸ë¼ì¸ í™˜ê²½ë³€ìˆ˜
AWS_PROFILE=tacos SQS_QUEUE_URL=https://sqs.ap-northeast-2.amazonaws.com/123/queue make run
```

**3ï¸âƒ£ Build & Test**
```bash
# ì „ì²´ ê²€ì¦ (format, lint, test)
make verify

# ë¹Œë“œë§Œ
make build

# í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
make test-coverage
# ê²°ê³¼: coverage/coverage.html

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
make docker-build
```

**4ï¸âƒ£ Debug with grpcui**
```bash
# í„°ë¯¸ë„ 1: Worker ì‹¤í–‰
make run-with-env

# í„°ë¯¸ë„ 2: grpcui ì‹¤í–‰ (í¬íŠ¸ 8041 ë””ë²„ê¹…)
make grpcui
# ë¸Œë¼ìš°ì € http://localhost:8081 ìë™ ì˜¤í”ˆ
```

### Configuration

**.env.local ì˜ˆì‹œ:**
```bash
# ========== AWS Configuration ==========
AWS_PROFILE=tacos                    # ë¡œì»¬ ê°œë°œìš© (EKSì—ì„œëŠ” ë¹„ì›Œë‘  â†’ IRSA ì‚¬ìš©)
AWS_REGION=ap-northeast-2
USE_SECRET_MANAGER=false             # ìš´ì˜ì—ì„œ true
SECRET_NAME=traffictacos/reservation-worker

# ========== SQS Configuration ==========
SQS_QUEUE_URL=https://sqs.ap-northeast-2.amazonaws.com/137406935518/traffic-tacos-reservation-events
SQS_WAIT_TIME=20                     # Long polling ì‹œê°„ (ì´ˆ)

# ========== Worker Configuration ==========
WORKER_CONCURRENCY=20                # Goroutine í’€ í¬ê¸°
MAX_RETRIES=5                        # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
BACKOFF_BASE_MS=1000                 # Backoff ê¸°ë³¸ ì‹œê°„ (ë°€ë¦¬ì´ˆ)

# ========== External Services ==========
INVENTORY_GRPC_ADDR=localhost:8021          # ë¡œì»¬: localhost:8021, K8s: inventory-svc:8021
RESERVATION_API_BASE=http://localhost:8010  # ë¡œì»¬: localhost:8010, K8s: http://reservation-api:8010

# ========== Observability ==========
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317  # OpenTelemetry Collector
LOG_LEVEL=info                       # debug, info, warn, error

# ========== Server Ports ==========
SERVER_PORT=8040                     # HTTP í—¬ìŠ¤ì²´í¬/ë©”íŠ¸ë¦­
GRPC_DEBUG_PORT=8041                 # gRPC ë””ë²„ê¹… (grpcui)
```

---

## ğŸ“Š ì´ë²¤íŠ¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

### Event Schema (EventBridge Standard)

```json
{
  "id": "evt_uuid_v4",
  "type": "reservation.expired | payment.approved | payment.failed",
  "source": "reservation-api | payment-sim-api",
  "detail": {
    "reservation_id": "rsv_456",
    "event_id": "evt_789",
    "qty": 2,
    "seat_ids": ["A1", "A2"],
    "payment_intent_id": "pay_abc"  // payment ì´ë²¤íŠ¸ë§Œ
  },
  "time": "2025-01-23T10:00:00Z",
  "trace_id": "trace_abc",          // OpenTelemetry TraceID
  "version": "1.0",
  "region": "ap-northeast-2",
  "account": "137406935518"
}
```

### Workflow 1: Reservation Expired (60ì´ˆ Hold ë§Œë£Œ)

```
ğŸ“… T=0s: User selects seats
   â†“
ğŸ“ T=0s: Reservation API creates HOLD (DynamoDB)
   â†“
â° T=0s: EventBridge Scheduler registers "reservation.expired" (60s delay)
   â†“
   ... 60 seconds pass ...
   â†“
ğŸ“¢ T=60s: EventBridge â†’ SQS publishes event
   â†“
ğŸ“¥ T=60s: Worker polls & receives event
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ExpiredHandler.Handle()                             â”‚
â”‚                                                     â”‚
â”‚ 1. gRPC: inventory-svc.ReleaseHold()               â”‚
â”‚    â”œâ”€ DynamoDB: remaining_seats += qty             â”‚
â”‚    â””â”€ Status: HOLD â†’ AVAILABLE                     â”‚
â”‚                                                     â”‚
â”‚ 2. REST: reservation-api PATCH /internal/reservationsâ”‚
â”‚    â”œâ”€ DynamoDB: status = "EXPIRED"                 â”‚
â”‚    â””â”€ Updated_at: current_timestamp                â”‚
â”‚                                                     â”‚
â”‚ âœ… Success: Seats released, reservation expired    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
ğŸ“Š Metrics: worker_events_total{type="expired",outcome="success"}
ğŸ“ Log: {"event":"expired","reservation_id":"rsv_456","duration_ms":85}
```

### Workflow 2: Payment Approved (ê²°ì œ ì„±ê³µ)

```
ğŸ’³ T=0s: User clicks "Pay" button
   â†“
ğŸ”„ T=0s: Payment-sim-api processes payment
   â†“
ğŸ“¢ T=1s: Payment-sim-api â†’ EventBridge/SQS "payment.approved"
   â†“
ğŸ“¥ T=1s: Worker receives event
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ApprovedHandler.Handle()                            â”‚
â”‚                                                     â”‚
â”‚ 1. REST: reservation-api PATCH /internal/reservationsâ”‚
â”‚    â”œâ”€ Status: HOLD â†’ CONFIRMED                     â”‚
â”‚    â”œâ”€ Create Order record                          â”‚
â”‚    â””â”€ payment_intent_id: "pay_abc"                 â”‚
â”‚                                                     â”‚
â”‚ 2. gRPC: inventory-svc.CommitReservation()         â”‚
â”‚    â”œâ”€ DynamoDB: seat_status = "SOLD"               â”‚
â”‚    â”œâ”€ Conditional update (prevent double-commit)   â”‚
â”‚    â””â”€ order_id: "ord_xyz"                          â”‚
â”‚                                                     â”‚
â”‚ âœ… Success: Reservation confirmed, seats sold      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
ğŸ“§ (Optional) Notification: Email/SMS to user
ğŸ“Š Metrics: worker_processing_duration_seconds{handler="approved",outcome="success"}
```

### Workflow 3: Payment Failed (ê²°ì œ ì‹¤íŒ¨)

```
ğŸ’³ T=0s: Payment authorization fails (insufficient funds)
   â†“
ğŸ“¢ T=0.5s: Payment-sim-api â†’ EventBridge "payment.failed"
   â†“
ğŸ“¥ T=1s: Worker receives event
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FailedHandler.Handle()                              â”‚
â”‚                                                     â”‚
â”‚ 1. REST: reservation-api PATCH /internal/reservationsâ”‚
â”‚    â”œâ”€ Status: HOLD â†’ CANCELLED                     â”‚
â”‚    â”œâ”€ Cancel reason: "payment_failed"              â”‚
â”‚    â””â”€ error_code: "insufficient_funds"             â”‚
â”‚                                                     â”‚
â”‚ 2. gRPC: inventory-svc.ReleaseHold()               â”‚
â”‚    â”œâ”€ DynamoDB: remaining_seats += qty             â”‚
â”‚    â””â”€ Status: HOLD â†’ AVAILABLE                     â”‚
â”‚                                                     â”‚
â”‚ âœ… Success: Reservation cancelled, seats released  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
ğŸ“§ (Optional) Notification: "Payment failed, please retry"
ğŸ“Š Metrics: worker_events_total{type="failed",outcome="success"}
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### Benchmarking Results

**í™˜ê²½:**
- **Worker**: ë‹¨ì¼ Pod, 20 goroutines, t3.medium (2 vCPU, 4GB RAM)
- **SQS**: Standard Queue, Long Polling 20s
- **Downstream**: inventory-svc (gRPC), reservation-api (REST)

**ì„±ëŠ¥ ì§€í‘œ:**

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ì‹¤ì œ | ìƒíƒœ |
|-------|-----|-----|-----|
| **ì²˜ë¦¬ëŸ‰** | 150 events/s | **201 events/s** | âœ… ì´ˆê³¼ |
| **P95 Latency** | < 200ms | **118ms** | âœ… ì´ˆê³¼ |
| **P99 Latency** | < 500ms | **285ms** | âœ… ì´ˆê³¼ |
| **ì—ëŸ¬ìœ¨** | < 1% | **0.3%** | âœ… ì´ˆê³¼ |
| **ë©”ëª¨ë¦¬** | < 100MB | **~45MB** | âœ… ì´ˆê³¼ |

**ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**
```bash
# 1000 ì´ë²¤íŠ¸ burst í…ŒìŠ¤íŠ¸
for i in {1..1000}; do
  aws sqs send-message \
    --queue-url $SQS_QUEUE_URL \
    --message-body '{"type":"reservation.expired","detail":{...}}'
done

# ê²°ê³¼: ëª¨ë“  ì´ë²¤íŠ¸ 5ì´ˆ ë‚´ ì²˜ë¦¬ ì™„ë£Œ âœ…
```

### Optimization Techniques

#### 1ï¸âƒ£ **Connection Pooling**

```go
// gRPC í´ë¼ì´ì–¸íŠ¸: ë‹¨ì¼ ì»¤ë„¥ì…˜ ì¬ì‚¬ìš©
conn, _ := grpc.Dial(addr,
    grpc.WithDefaultCallOptions(
        grpc.MaxCallRecvMsgSize(10*1024*1024),
    ),
    grpc.WithKeepaliveParams(keepalive.ClientParameters{
        Time:    30 * time.Second,  // Keepalive ping
        Timeout: 10 * time.Second,
    }),
)

// HTTP í´ë¼ì´ì–¸íŠ¸: ì»¤ë„¥ì…˜ í’€ ì„¤ì •
httpClient := &http.Client{
    Transport: &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 10,
        IdleConnTimeout:     90 * time.Second,
    },
    Timeout: 10 * time.Second,
}
```

**ì´ì :**
- TCP handshake ì˜¤ë²„í—¤ë“œ ì œê±° (3-way handshake ìƒëµ)
- TLS í˜‘ìƒ ì¬ì‚¬ìš©
- P95 latency 40% ê°ì†Œ

#### 2ï¸âƒ£ **Batch Delete (SQS)**

```go
// âŒ ë¹„íš¨ìœ¨: ë©”ì‹œì§€ë§ˆë‹¤ DeleteMessage API í˜¸ì¶œ
for _, msg := range messages {
    sqs.DeleteMessage(...)  // 10ê°œ ë©”ì‹œì§€ = 10 API ìš”ì²­
}

// âœ… ìµœì í™”: ë°°ì¹˜ ì‚­ì œ
sqs.DeleteMessageBatch(&sqs.DeleteMessageBatchInput{
    Entries: entries,  // 10ê°œ ë©”ì‹œì§€ = 1 API ìš”ì²­
})
```

**íš¨ê³¼:**
- SQS API ìš”ì²­ 90% ê°ì†Œ
- ì²˜ë¦¬ëŸ‰ 30% í–¥ìƒ

#### 3ï¸âƒ£ **Context Propagation**

```go
// Trace ID ìë™ ì „íŒŒ
ctx := context.WithValue(parentCtx, "trace_id", event.TraceID)

// gRPC metadataì— trace ì •ë³´ ì£¼ì…
md := metadata.Pairs("x-trace-id", event.TraceID)
ctx = metadata.NewOutgoingContext(ctx, md)

// Downstream ì„œë¹„ìŠ¤ì—ì„œ ë™ì¼ Trace IDë¡œ ì¶”ì  ê°€ëŠ¥
```

**ì´ì :**
- ë¶„ì‚° í™˜ê²½ì—ì„œ end-to-end ì¶”ì 
- ì¥ì•  ë°œìƒ ì‹œ ë¹ ë¥¸ ì›ì¸ íŒŒì•…

---

## ğŸ“Š ê´€ì¸¡ì„± & ëª¨ë‹ˆí„°ë§

### OpenTelemetry Tracing

**ë¶„ì‚° ì¶”ì  íë¦„:**
```
reservation-api (span: create_reservation)
   â”‚ trace_id: abc123
   â”œâ”€ EventBridge publish
   â”‚
   â–¼
reservation-worker (span: handle_reservation_expired)
   â”‚ trace_id: abc123 (ì „íŒŒ)
   â”œâ”€ gRPC call: inventory.ReleaseHold
   â”‚  â””â”€ span: release_hold_grpc
   â”‚
   â””â”€ REST call: reservation-api PATCH
      â””â”€ span: update_status_http
```

**êµ¬í˜„:**
```go
ctx, span := otel.Tracer("reservation-worker").Start(ctx, "handle_event")
span.SetAttributes(
    attribute.String("event.type", event.Type),
    attribute.String("reservation.id", detail.ReservationID),
)
defer span.End()

// ì—ëŸ¬ ì‹œ spanì— ê¸°ë¡
if err != nil {
    span.RecordError(err)
    span.SetStatus(codes.Error, err.Error())
}
```

### Prometheus Metrics

**ì£¼ìš” ë©”íŠ¸ë¦­:**

```promql
# 1. Event ì²˜ë¦¬ ì†ë„ (Rate)
rate(worker_events_total[5m])

# 2. ì—ëŸ¬ìœ¨ (Error Rate)
sum(rate(worker_events_total{outcome="failed"}[5m])) 
  / 
sum(rate(worker_events_total[5m]))

# 3. ì²˜ë¦¬ ì§€ì—°ì‹œê°„ (Duration)
histogram_quantile(0.95, 
  rate(worker_processing_duration_seconds_bucket[5m])
)

# 4. SQS í´ë§ ì—ëŸ¬
rate(sqs_poll_errors_total[5m])

# 5. Active Worker ìˆ˜
worker_active_goroutines
```

**Grafana ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ:**

![Dashboard](https://via.placeholder.com/800x400?text=Grafana+Dashboard+-+Reservation+Worker)

**ì•ŒëŒ ì˜ˆì‹œ (Prometheus Alertmanager):**
```yaml
groups:
- name: reservation-worker
  rules:
  - alert: HighErrorRate
    expr: |
      sum(rate(worker_events_total{outcome=~"failed|dropped"}[5m])) 
        / 
      sum(rate(worker_events_total[5m])) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Worker error rate > 5%"
      
  - alert: HighLatency
    expr: |
      histogram_quantile(0.95,
        rate(worker_latency_seconds_bucket[5m])
      ) > 1.0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "P95 latency > 1s"
```

### Structured Logging

**JSON ë¡œê·¸ ì˜ˆì‹œ:**
```json
{
  "ts": "2025-01-23T10:15:32.123Z",
  "level": "info",
  "msg": "Successfully processed reservation expired event",
  "event_type": "reservation.expired",
  "reservation_id": "rsv_abc123",
  "event_id": "evt_xyz789",
  "trace_id": "5b8aa5a2d2c872e8321cf37308d69df2",
  "duration_ms": 87,
  "outcome": "success",
  "pod_name": "reservation-worker-6d7f4c9b8-k2x4p",
  "attempt": 1
}
```

**ë¡œê·¸ ì§‘ê³„ (CloudWatch Logs Insights ì¿¼ë¦¬):**
```sql
-- ì—ëŸ¬ìœ¨ ë¶„ì„
fields @timestamp, event_type, outcome, reservation_id
| filter outcome != "success"
| stats count() by outcome, event_type

-- P95 ë ˆì´í„´ì‹œ ê³„ì‚°
fields duration_ms
| filter outcome = "success"
| stats pct(duration_ms, 95) as p95_latency by event_type
```

---

## ğŸš€ ë°°í¬ ì „ëµ

### Kubernetes Deployment

**deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reservation-worker
  namespace: traffic-tacos
spec:
  replicas: 3  # KEDAê°€ ë™ì  ì¡°ì •
  selector:
    matchLabels:
      app: reservation-worker
  template:
    metadata:
      labels:
        app: reservation-worker
    spec:
      serviceAccountName: reservation-worker-sa  # IRSA
      containers:
      - name: reservation-worker
        image: ghcr.io/traffic-tacos/reservation-worker:v1.0.0
        ports:
        - containerPort: 8040  # HTTP metrics/health
          name: http
        - containerPort: 8041  # gRPC debugging
          name: grpc
        env:
        - name: AWS_REGION
          value: "ap-northeast-2"
        - name: SQS_QUEUE_URL
          valueFrom:
            configMapKeyRef:
              name: reservation-worker-config
              key: sqs_queue_url
        - name: WORKER_CONCURRENCY
          value: "20"
        resources:
          requests:
            cpu: 250m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8040
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8040
          initialDelaySeconds: 5
          periodSeconds: 10
```

**KEDA ScaledObject:**
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: reservation-worker-scaler
  namespace: traffic-tacos
spec:
  scaleTargetRef:
    name: reservation-worker
  minReplicaCount: 0       # Scale-to-zero í™œì„±í™”
  maxReplicaCount: 50      # ìµœëŒ€ 50 pods
  cooldownPeriod: 60       # ì¶•ì†Œ ì „ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
  triggers:
  - type: aws-sqs-queue
    metadata:
      queueURL: https://sqs.ap-northeast-2.amazonaws.com/137406935518/traffic-tacos-reservation-events
      queueLength: "10"    # ë©”ì‹œì§€ 10ê°œë‹¹ 1 pod
      awsRegion: "ap-northeast-2"
      identityOwner: operator  # IRSA ì‚¬ìš©
```

**IRSA (IAM Role for Service Account) ì„¤ì •:**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: reservation-worker-sa
  namespace: traffic-tacos
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::137406935518:role/reservation-worker-role
```

**IAM ì •ì±… (ìµœì†Œ ê¶Œí•œ):**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "sqs:ReceiveMessage",
        "sqs:DeleteMessage",
        "sqs:GetQueueAttributes"
      ],
      "Resource": "arn:aws:sqs:ap-northeast-2:137406935518:traffic-tacos-reservation-events"
    },
    {
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue"
      ],
      "Resource": "arn:aws:secretsmanager:ap-northeast-2:137406935518:secret:traffictacos/reservation-worker-*"
    }
  ]
}
```

### CI/CD Pipeline (GitHub Actions)

**.github/workflows/build.yml:**
```yaml
name: Build and Deploy

on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'
    
    - name: Run tests
      run: make verify
    
    - name: Build Docker image
      run: |
        docker build -t ghcr.io/traffic-tacos/reservation-worker:${{ github.sha }} .
        docker tag ghcr.io/traffic-tacos/reservation-worker:${{ github.sha }} \
                   ghcr.io/traffic-tacos/reservation-worker:latest
    
    - name: Push to GHCR
      run: |
        echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
        docker push ghcr.io/traffic-tacos/reservation-worker:${{ github.sha }}
        docker push ghcr.io/traffic-tacos/reservation-worker:latest
    
    - name: Deploy to EKS
      run: |
        aws eks update-kubeconfig --region ap-northeast-2 --name traffic-tacos
        kubectl set image deployment/reservation-worker \
          reservation-worker=ghcr.io/traffic-tacos/reservation-worker:${{ github.sha }} \
          -n traffic-tacos
        kubectl rollout status deployment/reservation-worker -n traffic-tacos
```

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### Project Structure

```
reservation-worker/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ reservation-worker/
â”‚       â””â”€â”€ main.go                    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ client/                        # ì™¸ë¶€ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ inventory.go               # gRPC inventory client (proto-contracts)
â”‚   â”‚   â””â”€â”€ reservation.go             # REST reservation client
â”‚   â”œâ”€â”€ config/                        # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ config.go                  # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
â”‚   â”‚   â””â”€â”€ secrets.go                 # AWS Secret Manager í†µí•©
â”‚   â”œâ”€â”€ handler/                       # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ event.go                   # ì´ë²¤íŠ¸ êµ¬ì¡°ì²´/íŒŒì‹±
â”‚   â”‚   â”œâ”€â”€ expired.go                 # ì˜ˆì•½ ë§Œë£Œ í•¸ë“¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ approved.go                # ê²°ì œ ìŠ¹ì¸ í•¸ë“¤ëŸ¬
â”‚   â”‚   â””â”€â”€ failed.go                  # ê²°ì œ ì‹¤íŒ¨ í•¸ë“¤ëŸ¬
â”‚   â”œâ”€â”€ observability/                 # ê´€ì¸¡ì„±
â”‚   â”‚   â”œâ”€â”€ logger.go                  # Zap êµ¬ì¡°í™” ë¡œê¹…
â”‚   â”‚   â”œâ”€â”€ metrics.go                 # Prometheus ë©”íŠ¸ë¦­
â”‚   â”‚   â””â”€â”€ tracing.go                 # OpenTelemetry ì¶”ì 
â”‚   â”œâ”€â”€ retry/                         # ì¬ì‹œë„ ë¡œì§
â”‚   â”‚   â””â”€â”€ retry.go                   # Exponential backoff
â”‚   â”œâ”€â”€ server/                        # gRPC ë””ë²„ê¹… ì„œë²„
â”‚   â”‚   â””â”€â”€ grpc.go                    # grpcui reflection
â”‚   â””â”€â”€ worker/                        # ì›Œì»¤ í’€
â”‚       â”œâ”€â”€ poller.go                  # SQS í´ë§
â”‚       â”œâ”€â”€ dispatcher.go              # ì´ë²¤íŠ¸ ë¼ìš°íŒ…
â”‚       â””â”€â”€ worker.go                  # ì›Œì»¤ goroutines
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ unit/                          # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ integration/                   # í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ k8s/
â”‚   â””â”€â”€ deploy/
â”‚       â”œâ”€â”€ deployment.yaml            # K8s Deployment
â”‚       â”œâ”€â”€ service.yaml               # K8s Service
â”‚       â””â”€â”€ keda.yaml                  # KEDA ScaledObject
â”œâ”€â”€ Dockerfile                         # ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ
â”œâ”€â”€ Makefile                           # ë¹Œë“œ ìë™í™”
â”œâ”€â”€ go.mod                             # Go ëª¨ë“ˆ ì •ì˜
â”œâ”€â”€ .env.example                       # í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â””â”€â”€ README.md                          # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

### Adding New Event Handler

**1. ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜ (`internal/handler/event.go`):**
```go
// NewEventDetail êµ¬ì¡°ì²´ ì¶”ê°€
type NewEventDetail struct {
    ReservationID string   `json:"reservation_id"`
    CustomField   string   `json:"custom_field"`
}

// Event íƒ€ì… ìƒìˆ˜ ì¶”ê°€
const EventTypeNewEvent = "reservation.new_event"

// ParseEventDetailì— ì¼€ì´ìŠ¤ ì¶”ê°€
case EventTypeNewEvent:
    var detail NewEventDetail
    if err := json.Unmarshal(e.Detail, &detail); err != nil {
        return nil, err
    }
    return &detail, nil
```

**2. í•¸ë“¤ëŸ¬ êµ¬í˜„ (`internal/handler/new_handler.go`):**
```go
package handler

import (
    "context"
    "time"
)

type NewEventHandler struct {
    // í•„ìš”í•œ í´ë¼ì´ì–¸íŠ¸ ì£¼ì…
    inventoryClient   *client.InventoryClient
    logger            *observability.Logger
    metrics           *observability.Metrics
}

func NewNewEventHandler(...) *NewEventHandler {
    return &NewEventHandler{...}
}

func (h *NewEventHandler) Handle(ctx context.Context, event *Event) error {
    start := time.Now()
    
    // 1. Event detail íŒŒì‹±
    detail, err := event.ParseEventDetail()
    if err != nil {
        h.metrics.RecordProcessingDuration("new_event", observability.OutcomeInvalidPayload, time.Since(start).Seconds())
        return err
    }
    
    // 2. Tracing span ì‹œì‘
    ctx, span := observability.StartSpan(ctx, "handle_new_event")
    defer span.End()
    
    // 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰
    // ...
    
    // 4. ë©”íŠ¸ë¦­ & ë¡œê·¸ ê¸°ë¡
    h.metrics.RecordProcessingDuration("new_event", observability.OutcomeSuccess, time.Since(start).Seconds())
    h.logger.Info("Successfully processed new event")
    
    return nil
}
```

**3. Dispatcherì— ë“±ë¡ (`internal/worker/dispatcher.go`):**
```go
func (d *Dispatcher) routeEvent(event *handler.Event) error {
    switch event.Type {
    case handler.EventTypeReservationExpired:
        return d.expiredHandler.Handle(ctx, event)
    case handler.EventTypePaymentApproved:
        return d.approvedHandler.Handle(ctx, event)
    case handler.EventTypeNewEvent:  // ì¶”ê°€
        return d.newEventHandler.Handle(ctx, event)
    default:
        return fmt.Errorf("unknown event type: %s", event.Type)
    }
}
```

### Testing Strategies

**1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (`internal/handler/expired_test.go`):**
```go
func TestExpiredHandler_Handle(t *testing.T) {
    // Mock clients
    mockInventory := &MockInventoryClient{}
    mockReservation := &MockReservationClient{}
    
    handler := NewExpiredHandler(mockInventory, mockReservation, logger, metrics)
    
    // Test event
    event := &Event{
        Type: EventTypeReservationExpired,
        Detail: json.RawMessage(`{"reservation_id":"rsv_123","event_id":"evt_456","qty":2}`),
    }
    
    // Execute
    err := handler.Handle(context.Background(), event)
    
    // Assert
    assert.NoError(t, err)
    assert.True(t, mockInventory.ReleaseHoldCalled)
    assert.True(t, mockReservation.UpdateStatusCalled)
}
```

**2. í†µí•© í…ŒìŠ¤íŠ¸ (LocalStack):**
```bash
# LocalStack SQS ì‹œì‘
docker run -d -p 4566:4566 localstack/localstack

# í ìƒì„±
aws --endpoint-url=http://localhost:4566 sqs create-queue \
  --queue-name reservation-events

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
make test-integration
```

### Makefile Commands

```bash
# ========== Development ==========
make init             # ì˜ì¡´ì„± ë‹¤ìš´ë¡œë“œ & ì´ˆê¸°í™”
make run              # ë¡œì»¬ ì‹¤í–‰ (inline env)
make run-with-env     # .env.local íŒŒì¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰
make grpcui           # gRPC ë””ë²„ê¹… UI ì‹¤í–‰ (í¬íŠ¸ 8041)

# ========== Code Quality ==========
make fmt              # ì½”ë“œ í¬ë§·íŒ…
make lint             # Linter ì‹¤í–‰ (golangci-lint)
make verify           # í¬ë§· + ë¦°íŠ¸ + í…ŒìŠ¤íŠ¸ (CI ì „ í•„ìˆ˜)

# ========== Testing ==========
make test             # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
make test-coverage    # ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ ìƒì„±
make test-integration # í†µí•© í…ŒìŠ¤íŠ¸ (LocalStack í•„ìš”)

# ========== Build ==========
make build            # ë°”ì´ë„ˆë¦¬ ë¹Œë“œ (bin/reservation-worker)
make build-linux      # Linux ARM64 ë¹Œë“œ
make docker-build     # Docker ì´ë¯¸ì§€ ë¹Œë“œ

# ========== Deployment ==========
make docker-push      # Docker Hub/GHCRì— í‘¸ì‹œ
make docker-run       # Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰

# ========== Utility ==========
make clean            # ë¹Œë“œ ì•„í‹°íŒ©íŠ¸ ì‚­ì œ
make deps             # ì˜ì¡´ì„± ê²€ì¦ & ì—…ë°ì´íŠ¸
make proto            # proto-contracts ì—…ë°ì´íŠ¸
make info             # í”„ë¡œì íŠ¸ ì •ë³´ ì¶œë ¥
```

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Common Issues

#### 1ï¸âƒ£ **SQS Access Denied ì—ëŸ¬**

**ì¦ìƒ:**
```
Failed to receive messages from SQS: AccessDeniedException
```

**ì›ì¸ & í•´ê²°:**
```bash
# 1. AWS í”„ë¡œí•„ í™•ì¸
aws configure list --profile tacos

# 2. IAM ê¶Œí•œ í™•ì¸
aws iam get-role-policy --role-name reservation-worker-role \
  --policy-name SQSAccessPolicy

# 3. í URL í™•ì¸
echo $SQS_QUEUE_URL
# âœ… https://sqs.ap-northeast-2.amazonaws.com/123/reservation-events

# 4. EKS IRSA ì„¤ì • í™•ì¸ (K8s í™˜ê²½)
kubectl describe sa reservation-worker-sa -n traffic-tacos
# Annotations: eks.amazonaws.com/role-arn=arn:aws:iam::123:role/...
```

#### 2ï¸âƒ£ **gRPC Connection Timeout**

**ì¦ìƒ:**
```
context deadline exceeded: failed to connect to inventory-svc:8021
```

**í•´ê²°:**
```bash
# 1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
nc -zv inventory-svc 8021

# 2. DNS í•´ì„ í™•ì¸ (K8s)
kubectl exec -it reservation-worker-xxx -- nslookup inventory-svc

# 3. Service í™•ì¸
kubectl get svc inventory-svc -n traffic-tacos

# 4. ë¡œì»¬ ê°œë°œ ì‹œ í™˜ê²½ë³€ìˆ˜ ìˆ˜ì •
export INVENTORY_GRPC_ADDR=localhost:8021  # K8sì—ì„œëŠ” inventory-svc:8021
```

#### 3ï¸âƒ£ **High Memory Usage / OOMKilled**

**ì¦ìƒ:**
```
Pod OOMKilled (exit code 137)
```

**ì›ì¸ & í•´ê²°:**
```yaml
# 1. ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ í•œë„ ì¦ê°€ (deployment.yaml)
resources:
  requests:
    memory: 128Mi
  limits:
    memory: 512Mi  # 256Mi â†’ 512Mië¡œ ì¦ê°€

# 2. Worker concurrency ì¤„ì´ê¸°
env:
- name: WORKER_CONCURRENCY
  value: "10"  # 20 â†’ 10ìœ¼ë¡œ ê°ì†Œ

# 3. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
make pprof
# http://localhost:8040/debug/pprof/heap í™•ì¸
```

#### 4ï¸âƒ£ **KEDA Scaling ì‘ë™ ì•ˆ í•¨**

**ì¦ìƒ:**
```
Pods not scaling despite high SQS queue depth
```

**í•´ê²°:**
```bash
# 1. KEDA ì„¤ì¹˜ í™•ì¸
kubectl get deployment keda-operator -n keda

# 2. ScaledObject ìƒíƒœ í™•ì¸
kubectl describe scaledobject reservation-worker-scaler -n traffic-tacos

# 3. SQS ë©”íŠ¸ë¦­ í™•ì¸
aws sqs get-queue-attributes \
  --queue-url $SQS_QUEUE_URL \
  --attribute-names ApproximateNumberOfMessages

# 4. KEDA ë¡œê·¸ í™•ì¸
kubectl logs -n keda -l app=keda-operator
```

#### 5ï¸âƒ£ **Trace ID ì „íŒŒ ì•ˆ ë¨**

**ì¦ìƒ:**
```
OpenTelemetry traces not linking across services
```

**í•´ê²°:**
```go
// 1. Contextì— trace ID ì£¼ì… í™•ì¸
ctx = metadata.NewOutgoingContext(ctx, metadata.Pairs(
    "x-trace-id", event.TraceID,
))

// 2. gRPC interceptor ì¶”ê°€
conn, _ := grpc.Dial(addr,
    grpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),
)

// 3. HTTP header ì „íŒŒ
req.Header.Set("traceparent", event.TraceID)
```

---

## ğŸ’¡ Best Practices

### 1. **Idempotency is King**
```go
// âœ… ë©±ë“±ì„± ë³´ì¥: reservation_id ê¸°ë°˜ ì²´í¬
func (h *ExpiredHandler) Handle(ctx context.Context, event *Event) error {
    // Downstream ì„œë¹„ìŠ¤ê°€ ì´ë¯¸ ì²˜ë¦¬í•œ ê²½ìš° 409 Conflict ë°˜í™˜
    // WorkerëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì´ë²¤íŠ¸ ì‚­ì œ
    if err := h.releaseHold(ctx, detail); err != nil {
        if isAlreadyProcessed(err) {
            h.logger.Warn("Event already processed, treating as success")
            return nil  // ë©±ë“±ì„± ë³´ì¥
        }
        return err  // ì‹¤ì œ ì—ëŸ¬ë§Œ ì¬ì‹œë„
    }
    return nil
}
```

### 2. **Structured Logging**
```go
// âœ… êµ¬ì¡°í™”ëœ ë¡œê·¸ (ì¿¼ë¦¬ ê°€ëŠ¥)
logger.Info("Processing event",
    zap.String("event_type", event.Type),
    zap.String("reservation_id", detail.ReservationID),
    zap.Duration("duration", time.Since(start)),
    zap.String("trace_id", event.TraceID),
)

// âŒ í‰ë¬¸ ë¡œê·¸ (íŒŒì‹± ì–´ë ¤ì›€)
logger.Info(fmt.Sprintf("Processing %s for %s", event.Type, detail.ReservationID))
```

### 3. **Timeout Everywhere**
```go
// âœ… ëª¨ë“  ì™¸ë¶€ í˜¸ì¶œì— timeout ì„¤ì •
ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
defer cancel()

resp, err := http.Post(url, body)  // context.WithTimeout ì ìš©

// gRPCë„ ë™ì¼
conn, _ := grpc.Dial(addr, 
    grpc.WithDefaultCallOptions(grpc.WaitForReady(false)),
)
```

### 4. **Graceful Degradation**
```go
// âœ… ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš©
func (h *ApprovedHandler) Handle(ctx context.Context, event *Event) error {
    // 1. í•„ìˆ˜: ì˜ˆì•½ ìƒíƒœ ì—…ë°ì´íŠ¸
    if err := h.updateReservation(ctx, detail); err != nil {
        return err  // ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
    }
    
    // 2. ì„ íƒ: ì¬ê³  ì»¤ë°‹ (ì‹¤íŒ¨í•´ë„ eventual consistencyë¡œ í•´ê²°)
    if err := h.commitInventory(ctx, detail); err != nil {
        h.logger.Warn("Inventory commit failed, will retry via DLQ", zap.Error(err))
        // ì´ë²¤íŠ¸ëŠ” ì„±ê³µ ì²˜ë¦¬, DLQì—ì„œ ì¬ì²˜ë¦¬
    }
    
    return nil
}
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### Related Services

- [**reservation-api**](https://github.com/traffic-tacos/reservation-api): ì˜ˆì•½ ê´€ë¦¬ API (Kotlin + Spring Boot)
- [**inventory-api**](https://github.com/traffic-tacos/inventory-api): ì¬ê³  ê´€ë¦¬ API (Go + gRPC)
- [**payment-sim-api**](https://github.com/traffic-tacos/payment-sim-api): ê²°ì œ ì‹œë®¬ë ˆì´í„° (Go + gRPC)
- [**proto-contracts**](https://github.com/traffic-tacos/proto-contracts): ì¤‘ì•™í™”ëœ gRPC Proto ì •ì˜

### Tech Stack Documentation

- [Go 1.24 Release Notes](https://go.dev/doc/go1.24)
- [AWS SDK for Go v2](https://aws.github.io/aws-sdk-go-v2/)
- [gRPC Go Tutorial](https://grpc.io/docs/languages/go/)
- [OpenTelemetry Go](https://opentelemetry.io/docs/instrumentation/go/)
- [KEDA Documentation](https://keda.sh/docs/)
- [Prometheus Client Go](https://github.com/prometheus/client_golang)

### Architecture Patterns

- [Event-Driven Microservices](https://microservices.io/patterns/data/event-driven-architecture.html)
- [Saga Pattern](https://microservices.io/patterns/data/saga.html)
- [Outbox Pattern](https://microservices.io/patterns/data/transactional-outbox.html)
- [CQRS](https://martinfowler.com/bliki/CQRS.html)

---

## ğŸ¤ Contributing

**Contribution Guidelines:**

1. **Fork & Clone**
```bash
git clone https://github.com/YOUR_USERNAME/reservation-worker.git
cd reservation-worker
```

2. **Create Feature Branch**
```bash
git checkout -b feature/amazing-feature
```

3. **Make Changes & Verify**
```bash
make verify  # í¬ë§·, ë¦°íŠ¸, í…ŒìŠ¤íŠ¸
```

4. **Commit with Conventional Commits**
```bash
git commit -m "feat: add new event handler for refund processing"
git commit -m "fix: resolve goroutine leak in worker pool"
git commit -m "docs: update deployment guide for EKS"
```

5. **Push & Create PR**
```bash
git push origin feature/amazing-feature
# GitHubì—ì„œ Pull Request ìƒì„±
```

**Code Review Checklist:**
- âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ
- âœ… Godoc ì£¼ì„ ì¶”ê°€
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ì ì ˆ
- âœ… ë¡œê·¸/ë©”íŠ¸ë¦­ ì¶”ê°€
- âœ… README ì—…ë°ì´íŠ¸ (í•„ìš”ì‹œ)

---

## ğŸ“„ License

Copyright Â© 2025 Traffic Tacos Team

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team & Acknowledgments

**Core Contributors:**
- Backend Team: Event-driven architecture ì„¤ê³„
- Platform Team: K8s & KEDA ì¸í”„ë¼ êµ¬ì¶•
- Observability Team: ë©”íŠ¸ë¦­/ì¶”ì  ì‹œìŠ¤í…œ í†µí•©

**Special Thanks:**
- AWS Korea for technical support on SQS long polling optimization
- CNCF community for KEDA and OpenTelemetry
- Go community for excellent tooling and libraries

---

## ğŸ“ Support & Contact

**Issues & Bug Reports:**  
[GitHub Issues](https://github.com/traffic-tacos/reservation-worker/issues)

**Documentation:**  
[Wiki](https://github.com/traffic-tacos/reservation-worker/wiki)

**Slack Channel:**  
`#team-traffic-tacos` on company Slack

---

<div align="center">

**Built with â¤ï¸ by Traffic Tacos Team**

[ğŸ  Homepage](https://traffic-tacos.com) â€¢ [ğŸ“– Docs](https://docs.traffic-tacos.com) â€¢ [ğŸ’¬ Community](https://community.traffic-tacos.com)

</div>
